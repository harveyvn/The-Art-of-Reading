\documentclass[10pt,a4paper]{report}
%
%
%% IF YOU EXPERIENCE ANY PROBLEM WITH THIS TEMPLATE CONTACT DR. ALESSIO GAMBI
%
%
\usepackage[a4paper, total={6in, 10in}]{geometry}

\usepackage{titling}
\usepackage[utf8]{inputenc}

%%%% Machinery to draw the rating stars
\usepackage{tikz}
\usetikzlibrary{shapes.geometric}
\newcommand{\Stars}[2][fill=yellow,draw=orange]{\begin{tikzpicture}[baseline=-0.35em,#1]
\foreach \X in {1,...,5}
{\pgfmathsetmacro{\xfill}{min(1,max(1+#2-\X,0))}
\path (\X*1.1em,0) 
node[star,draw,star point height=0.25em,minimum size=1em,inner sep=0pt,
path picture={\fill (path picture bounding box.south west) 
rectangle  ([xshift=\xfill*1em]path picture bounding box.north west);}]{};
}
\end{tikzpicture}}
%%%% Machinery to draw the rating stars

\usepackage{fancyhdr}
\thispagestyle{fancy}
\pagestyle{fancy}

\usepackage{paralist}

\usepackage{titlesec}
\titleformat{\section}{\normalfont\fontsize{12}{15}\bfseries}{\thesection}{1em}{}

\usepackage[backend=bibtex8,style=numeric]{biblatex}
\addbibresource{biblio.bib}

\usepackage[english]{babel}
\usepackage{blindtext}

\renewcommand{\thesection}{\arabic{section}}

%%%% Related Work environments
\newcounter{RelatedWorkCounter}
\addtocounter{RelatedWorkCounter}{1}
\newcommand{\relatedwork}[3]{%
\paragraph{Paper:}\fullcite{#1}
\begin{compactdesc}
\item[- How:] #2
\item[- Why:] #3
\end{compactdesc}
\stepcounter{RelatedWorkCounter}
}

%%%% Critical question environments
\newcounter{QuestionCounter}
\addtocounter{QuestionCounter}{1}
\makeatletter
\newcommand{\criticalquestion}[1]{\def\criticalquestion@required{#1}\criticalquestion@opt}
\newcommand{\criticalquestion@opt}[1]{%
\paragraph{Q\theQuestionCounter: \criticalquestion@required}
#1%
\stepcounter{QuestionCounter}
}
\makeatother

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Meta Data:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\lhead{Vuong Nguyen}
\rhead{Topic: Basic Search/Paper 01}
\title{Search-Based Test Case Generation for Cyber-Physical Systems}
\author{Vuong Nguyen}
\date{\today}

\begin{document}
\begin{center}
\textbf{\thetitle}
\end{center}

%%%%%%%%%
% If your text is too long and you need to choose what part to cut down between Summary and Critical Discussion, always cut down on the Summary! We all have read the paper, so the really interesting part is your opinion!
%%%%%%%%%
\section{Summary}
%1/2 Page
The paper discusses the testing challenges of the Cyber-Physical Systems (CPSs) and then proposes a multi-objective search-based approach to solve them. CPSs refers to systems in which computational and physical layers are deeply integrated. There are several factors that make testing in CPSs challenging, such as complex networks, unpredictable behavior, and heterogeneous input data. The paper mentions simulation-based testing despite its own computational complexity.

In order to solve these issues, researchers introduced a new search-based approach which concentrates on three cost-eﬀectiveness measures: requirements coverage (RC) - how many requirements a speciﬁc test suite covers, test case similarity (TCS) - a measurement of similarity among test cases and test execution time (TET). There have been several relevant works discovered recently but none of them takes these measures into account. On a side note, the process obtained test cases is semi-automatic. 

The search-based approach consists of an algorithm based on the multi-objective search algorithms known as NSGA-II along with one crossover operator and three mutation operators (\emph{Mu\_BO}, \emph{Mu\_TS}, \emph{Mu\_TC}) with an aim to ﬁnd optimal reactive test cases. In order to indicate that the approach is superior in comparison to Random Search (RS), four case scenarios such as DC Engine, Tank System, UAV, and Cruise Control were selected and researchers validated their approach along with three mutation operators by addressing two research questions. The former speciﬁes the comparison between the approach and RS while the latter speciﬁes the comparison among the three mutation operators to ﬁnd the most efficient one. Regarding evaluation metrics, they chose the Hypervolume (HV) quality indicator to measure the requirement coverage volume. The indicator value is between 0 and 1 and a higher value implies better performance. In addition, the Shapiro-Wilk test, t-test, and Vargha and Delaney statistics were applied to determine the signiﬁcance and diﬀerences between test generations. The algorithm was then executed 100 times and for each iteration, its result was collected and analyzed. As a result, the performance of the approach exceeded the RS signiﬁcantly in all four case scenarios and \emph{Mu\_BO} demonstrated its performance, overtaking \emph{Mu\_TS} and \emph{Mu\_TC} by 75\%.\par
\section{Critical Discussion}
%1/2 Page

First of all, the paper was well written. In the abstract of the paper, it not only identiﬁed problems that the traditional model-based testing framework faced in CPSs but also proposed an alternative solution to address those problems successfully. In the same way, the paper provided compelling evidence to support their arguments on why their approach statistically outperformed RS in most of the given case studies. Furthermore, researchers attempted to make their paper approachable to the public by explaining basic concepts such as reactive test cases, requirements coverage, test case similarity, test execution time, test suite, etc. in detail. Therefore, it makes those terms comprehensible, even for people with minimal software testing experience. Finally, the goals of the experiment are identiﬁed clearly and measured mathematically by the authors. To elaborate, the description of objective measures such as requirements coverage, test case similarity, and test execution time is transparent and repeated throughout the paper. Researchers also presented several mathematical equations to deﬁne the cost and eﬀectiveness of those measures and all of them contributed to examination of optimal reactive test cases. Besides, thanks to four case scenarios, evaluation metrics such as Hypervolume quality indicator and other statistical analysis methods like t-test, results of experiments are reliable and adequate to justify the hypothesis.

On the other hand, algorithm parameters conﬁguration followed the recommendation by jMetal, which can result in misunderstanding for the audience. Although the authors described many conﬁguration parameters of the algorithm thoroughly, they missed the reason behind which can lead to confusion in its initial setup. Moreover, diﬀerent configurations can lead to diﬀerent result analysis. Thus, the research failed to report the system’s behavior and describe the eﬀect of the result after changing that parameter conﬁgurations. Last but not least, the authors did not explain why they chose Hypervolume as a quality indicator. Since other quality indicators such as Hypervolume, Epsilon, and Generalized Spread can apply to evaluate the quality of Pareto fronts [1], it will be better to have a rigorous explanation to support their choice over others.

To conclude, I’m almost convinced that a search-based approach is by far an ideal, pragmatic approach built on top of NSGA-II in search of the optimal reactive test cases for CPSs. 
\newpage 
% Paper rating, critical questions and related work sections must always appear on the second page of the summary

\section{Rating}
% Add here the overall rating of the paper (1 start is BAD, 5 starts is VERY GOOD). Please explain in one or two sentences the reason of your evaluation and whether you suggests the paper for the next edition of the seminar.
\Stars{4}

The paper has defined many testing concepts transparently and illustrated the empirical evaluation carefully with proper research questions and statistical methods. However, due to the minor limitations above, I would love to give this paper 4 stars.

Of course I highly recommend this paper for the next semester !


\section{Critical Questions}
% A least 2 questions here. If possible try to answer them or write down 
\criticalquestion{What is the role of selection operator in the experiment? How significantly does it affect to the evaluation' results?}% Optional answer follow
{In Section IV.B.2, the paper mentioned the Selection Operator but it missed explanation for the role of the operator.}

\criticalquestion{If the parameter configurations do not follow recommendation by jMetal, will the evaluation's result be different?}% Optional answer follow

\section{Related Work}
% Remember that you MUST list at least 4 related work here ! Fill the bib file will all the required information and build your bibliography before submitting the paper !
\paragraph{How many other papers did you considered during for the related work?} 4


% FIRST
\relatedwork%
% Put the citation key corresponding to the paper you selected here:
{arrieta2016test}%
% Explain how did you find the paper here (check the slides to see how you can effectively find related work papers)
{The main paper mentioned this paper in the reference section.}
% Explain why this paper is related here (do not just say, it has the same content or a similar title...)
{Providing the concept of Cyber-Physical Systems, Random Search, Reactive Test Cases and motivation why the paper selected the Random Search as a comparison baseline.}

% SECOND
\relatedwork
%% Put the cite key corresponding to the paper here:
{rahmati2013developing}%
%% Explain how did you find the paper here:
{Searched in Google with keywords Non-dominated sorting genetic algorithm and Binary Tournament Selection Operator.}
%% Explain why this paper is related here:
{Provide fundamental concepts of NSGA-II and Binary Tournament Selection Operator.}

% THIRD
\relatedwork
%% Put the cite key corresponding to the paper here:
{bringmann2008model}
%% Explain how did you find the paper here:
{Searched in Google with keywords Model-Based Testing, Automotive Systems.}
%% Explain why this paper is related here:
{Provide Model-based Testing in Practice and limitations in comparison to Simulation-based Testing.}

% FOURTH - AND PROBABLY LAST 
\relatedwork
%% Put the cite key corresponding to the paper here:
{wang2016practical}
%% Explain how did you find the paper here:
{The main paper mentioned this paper in the reference section.}
%% Explain why this paper is related here:
{Provide practical guide for selecting a suitable quality indicator with regards to different software engineering contexts.}

\section{References}

[1] \fullcite{wang2016practical}


\end{document}